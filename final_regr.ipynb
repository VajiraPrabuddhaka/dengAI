{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline\n",
    "#%run /home/vajira/PycharmProjects/dengAI/notebooks/myutil_regr.py\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "import importlib\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data and take first look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexed_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return set_index(df)\n",
    "\n",
    "def set_index(df):\n",
    "    df['yearweekofyear'] = df[['year','weekofyear']].\\\n",
    "                           apply(lambda x: str(x[0]) + str(x[1]).zfill(2), axis=1)\n",
    "    df.set_index('yearweekofyear', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_train = get_indexed_dataset('data/dengue_features_train.csv')\n",
    "dfy_train = get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "dfx_test = get_indexed_dataset('data/dengue_features_test.csv')\n",
    "# combine training features with training labels for data exploration later on\n",
    "dftrain = set_index(pd.merge(dfx_train, dfy_train))\n",
    "#dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with NaN on both training and test datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dftrain.dtypes\n",
    "#dftrain.isnull().sum()\n",
    "#dftest.isnull().sum()\n",
    "# Will stack the train and test datasets to treat all NaN values together\n",
    "# Need to add bogus total_cases column to test dataset so the files can be easily concatenated\n",
    "# update total_cases = -1 to easily identify the records for later split data to original partitions\n",
    "dfx_test['total_cases'] = -1\n",
    "dfall = set_index(pd.concat((dftrain, dfx_test), axis=0))\n",
    "dfall.sort_index(axis=0, inplace=True)\n",
    "#dfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                                       0\nyear                                       0\nweekofyear                                 0\nweek_start_date                            0\nndvi_ne                                  237\nndvi_nw                                   63\nndvi_se                                   23\nndvi_sw                                   23\nprecipitation_amt_mm                      15\nreanalysis_air_temp_k                     12\nreanalysis_avg_temp_k                     12\nreanalysis_dew_point_temp_k               12\nreanalysis_max_air_temp_k                 12\nreanalysis_min_air_temp_k                 12\nreanalysis_precip_amt_kg_per_m2           12\nreanalysis_relative_humidity_percent      12\nreanalysis_sat_precip_amt_mm              15\nreanalysis_specific_humidity_g_per_kg     12\nreanalysis_tdtr_k                         12\nstation_avg_temp_c                        55\nstation_diur_temp_rng_c                   55\nstation_max_temp_c                        23\nstation_min_temp_c                        23\nstation_precip_mm                         27\ntotal_cases                                0\ndtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfall.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nan_to_week_mean(df_with_nans):\n",
    "\n",
    "    cityweek_mean = df_with_nans.groupby(['city','weekofyear']).mean().to_dict()\n",
    "    # here's how we'd retrive mean ndvi_ne for city of iquito, week 1\n",
    "    # cityweek_mean['ndvi_ne'][('iq',1)]  \n",
    "\n",
    "    df_clean = df_with_nans.copy()\n",
    "    \n",
    "    # row is index to row\n",
    "    # cols is series with series index = dataframe column name\n",
    "    #                     series value = dataframe column value \n",
    "    #\n",
    "    skip_columns = set(['city','weekofyear','week_start_date','total_cases'])\n",
    "\n",
    "    # process iquito first\n",
    "    where = df_with_nans['city'] == 'iq'\n",
    "    for (row, cols) in df_with_nans[where].iterrows():\n",
    "        for idx in cols.index:\n",
    "            if pd.isnull(cols[idx]):\n",
    "                 #print('In rows {}, found null for field {}'.format(row, idx))\n",
    "                 if idx not in skip_columns: \n",
    "                      # there are no values for weekofyear = 53\n",
    "                      week_of_year = min(52, cols['weekofyear'])\n",
    "                      df_clean.loc[row, idx] = cityweek_mean[idx][('iq', week_of_year)]\n",
    "\n",
    "    # process san juan\n",
    "    where = df_with_nans['city'] == 'sj'\n",
    "    for (row, cols) in df_with_nans[where].iterrows():\n",
    "        for idx in cols.index:\n",
    "            if pd.isnull(cols[idx]):\n",
    "                 #print('In rows {}, found null for field {}'.format(row, idx))\n",
    "                 if idx not in skip_columns: \n",
    "                      # there are no values for weekofyear = 53\n",
    "                      week_of_year = min(52, cols['weekofyear'])\n",
    "                      df_clean.loc[row, idx] = cityweek_mean[idx][('sj', week_of_year)]\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                                     0\nyear                                     0\nweekofyear                               0\nweek_start_date                          0\nndvi_ne                                  0\nndvi_nw                                  0\nndvi_se                                  0\nndvi_sw                                  0\nprecipitation_amt_mm                     0\nreanalysis_air_temp_k                    0\nreanalysis_avg_temp_k                    0\nreanalysis_dew_point_temp_k              0\nreanalysis_max_air_temp_k                0\nreanalysis_min_air_temp_k                0\nreanalysis_precip_amt_kg_per_m2          0\nreanalysis_relative_humidity_percent     0\nreanalysis_sat_precip_amt_mm             0\nreanalysis_specific_humidity_g_per_kg    0\nreanalysis_tdtr_k                        0\nstation_avg_temp_c                       0\nstation_diur_temp_rng_c                  0\nstation_max_temp_c                       0\nstation_min_temp_c                       0\nstation_precip_mm                        0\ntotal_cases                              0\ndtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfall = set_nan_to_week_mean(dfall.copy())\n",
    "dfall.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_city(df):\n",
    "    return df[df['city']=='iq'], df[df['city']=='sj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfall_iq, dfall_sj = split_dataset_by_city(dfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "def drop_columns(df):\n",
    "    df.drop(['city','year','week_start_date'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "dfall_iq = drop_columns(dfall_iq.copy())\n",
    "dfall_sj = drop_columns(dfall_sj.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore training and test partitions (now that NaNs have been properly filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain_iq = dfall_iq[dfall_iq['total_cases']>0].copy()     # total_cases was set to -1 for test partition\n",
    "dftrain_sj = dfall_sj[dfall_sj['total_cases']>0].copy()     # total_cases was set to -1 for test partition\n",
    "\n",
    "dftest_iq = dfall_iq[dfall_iq['total_cases']<0].copy()\n",
    "dftest_sj = dfall_sj[dfall_sj['total_cases']<0].copy()\n",
    "dftest_iq.drop('total_cases', axis=1, inplace=True)\n",
    "dftest_sj.drop('total_cases', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess train data\n",
    "\n",
    "\n",
    "def preprocess(df, timesteps=1):\n",
    "\n",
    "    # step 4: split array into features (starting at col 5) and labels \n",
    "    X = df.values[:,:-1].astype('float32')\n",
    "    y = df.values[:,-1].reshape(X.shape[0],1)\n",
    "    \n",
    "    # step 5: normalize all features \n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # shifts features one row at a time and pads them to the left of existing feature set\n",
    "    feature_count = X.shape[1]\n",
    "    X_scaled= X_scaled[:-1,:feature_count] \n",
    "    for i in range(1, timesteps):\n",
    "        leftadd = X_scaled[:-1,:feature_count] \n",
    "        X_scaled = np.concatenate((leftadd, X_scaled[1:,:]), axis=1)\n",
    "\n",
    "    return np.concatenate((X_scaled, y[timesteps:]), axis=1) \n",
    "\n",
    "# preprocess test data\n",
    "def preprocess_test(df_train, df_test, timesteps=1):\n",
    "\n",
    "    df_test_rowcount = df_test.shape[0]\n",
    "\n",
    "    # will append training data, which preceeds test data in time\n",
    "    # so we can create sequences using previous periods data for our predictions\n",
    "    # just like we did during training \n",
    "    Xtrain = df_train.values[:,:-1].astype('float32')\n",
    "\n",
    "    X = np.concatenate((Xtrain, df_test.values.astype('float32')), axis=0)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # shifts features one row at a time and pads them to the left of existing feature set\n",
    "    feature_count = X.shape[1]\n",
    "    X_scaled= X_scaled[:-1,:feature_count] \n",
    "    for i in range(1, timesteps):\n",
    "        leftadd = X_scaled[:-1,:feature_count] \n",
    "        X_scaled = np.concatenate((leftadd, X_scaled[1:,:]), axis=1)\n",
    "\n",
    "    return X_scaled[-df_test_rowcount:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# split dataset into test and validation partitions\n",
    "def prep_regr_run(city_data):\n",
    "    \n",
    "    X = city_data[:,:-1]\n",
    "    y = city_data[:,-1]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "\n",
    "def regr_run(nptrain, poly_degree=1, exploring=False):\n",
    "    \n",
    "    # create partitions for training\n",
    "    X_train, X_valid, y_train, y_valid = prep_regr_run(nptrain)\n",
    "        \n",
    "    if poly_degree > 1:\n",
    "        poly = PolynomialFeatures(poly_degree, interaction_only=True)\n",
    "        X_train = poly.fit_transform(X_train)\n",
    "        X_valid = poly.fit_transform(X_valid)\n",
    "        \n",
    "    if exploring: print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    # Create linear regression object\n",
    "    #regr = linear_model.LinearRegression()\n",
    "    #regr = linear_model.Ridge(alpha = .5)\n",
    "    #regr = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "    #regr = linear_model.Lasso(alpha = .1)\n",
    "    #regr = linear_model.LassoLars(alpha = .1)\n",
    "    regr = linear_model.BayesianRidge()\n",
    "    #regr = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    print(\"fuck\")\n",
    "    print(X_train.shape)\n",
    "    #regr = NegativeBinomial(y_train,X_train)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "    #regr.fit()\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_valid)\n",
    "\n",
    "    # The coefficients\n",
    "    #print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean absolute error: %.2f\"\n",
    "          % mean_absolute_error(y_valid, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_valid, y_pred))\n",
    "    \n",
    "    return regr\n",
    "\n",
    "# Get test dataset, create predictions and save them in the proper submission file format\n",
    "def regr_predict_and_save(df_iq, model_iq, ts_iq, df_sj, model_sj, ts_sj, dftest_iq, dftest_sj, filename):\n",
    "  \n",
    "    nptest_iq = preprocess_test(df_iq.copy(), dftest_iq, ts_iq)\n",
    "    nptest_sj = preprocess_test(df_sj.copy(), dftest_sj, ts_sj)\n",
    "\n",
    "    yhat_iq = model_iq.predict(nptest_iq)\n",
    "    yhat_sj = model_sj.predict(nptest_sj)\n",
    "    \n",
    "    yhat_iq = yhat_iq.reshape(156,1)\n",
    "    yhat_sj = yhat_sj.reshape(260,1)\n",
    "    \n",
    "    print(yhat_iq.shape)\n",
    "    print(yhat_sj.shape)\n",
    "\n",
    "    dfsubm = pd.read_csv('data/submission_format.csv')\n",
    "    npsubm_sj = np.concatenate((dfsubm[dfsubm['city']=='sj'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_sj.round().astype('int64')), axis=1)\n",
    "    npsubm_iq = np.concatenate((dfsubm[dfsubm['city']=='iq'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_iq.round().astype('int64')), axis=1)\n",
    "    dfresults = pd.DataFrame(np.concatenate((npsubm_sj, npsubm_iq), axis=0), columns=dfsubm.columns)\n",
    "    \n",
    "    dfresults['total_cases'] = dfresults['total_cases'].clip(lower=0)\n",
    "    \n",
    "    dfresults.to_csv(filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 252) (136, 252) (276,) (136,)\nfuck\n(276, 252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 5.60\nVariance score: -0.01\n"
     ]
    }
   ],
   "source": [
    "periods_iq = 12   # best 12\n",
    "degree_iq = 1     # best 1\n",
    "nptrain_iq = preprocess(dftrain_iq.copy(), periods_iq)\n",
    "regr_iq = regr_run(nptrain_iq, degree_iq, exploring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 5250) (226, 5250) (456,) (226,)\nfuck\n(456, 5250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 15.25\nVariance score: 0.51\n"
     ]
    }
   ],
   "source": [
    "periods_sj = 250   # best 250\n",
    "degree_sj = 1      # best 1\n",
    "nptrain_sj = preprocess(dftrain_sj.copy(), periods_sj)\n",
    "regr_sj = regr_run(nptrain_sj, degree_sj, exploring=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test dataset and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 1)\n(260, 1)\n"
     ]
    }
   ],
   "source": [
    "regr_predict_and_save(dftrain_iq, regr_iq, periods_iq, dftrain_sj, regr_sj, periods_sj, dftest_iq, dftest_sj,\\\n",
    "                      datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
